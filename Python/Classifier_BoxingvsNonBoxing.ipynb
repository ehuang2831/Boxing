{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "features = pd.read_csv('../Data/FormattedTraining/ShadowVideosPhotosAugmented2/data.csv')\n",
    "labels = pd.read_csv('../Data/FormattedTraining/ShadowVideosPhotosAugmented2/target.csv')\n",
    "# Even spread of boxing vs non boxing (1-2)\n",
    "#features = pd.read_csv('../Data/FormattedTraining/ShadowVideos/data_even.csv')\n",
    "#labels = pd.read_csv('../Data/FormattedTraining/ShadowVideos/target_even.csv')\n",
    "feature_names = pd.read_csv('../Data/FormattedTraining/ShadowVideosPhotosAugmented2/feature_names.csv')\n",
    "label_names = ['Boxing','Not Boxing']\n",
    "\n",
    "labels = np.ravel(labels.as_matrix(columns=None))\n",
    "feature_names = np.ravel(feature_names.as_matrix(columns=None))\n",
    "features = features.as_matrix(columns=None)\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1179295,)\n",
      "(51,)\n",
      "(1179295, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Pelvis_x', 'R_Hip_x', 'R_Leg_x', 'R_Foot_x', 'L_Hip_x', 'L_Leg_x',\n",
       "       'L_Foot_x', 'Belly_x', 'Thorax_x', 'Neck_x', 'Head_x',\n",
       "       'L_Shoulder_x', 'L_Arm_x', 'L_Hand_x', 'R_Shoulder_x', 'R_Arm_x',\n",
       "       'R_Hand_x', 'Pelvis_y', 'R_Hip_y', 'R_Leg_y', 'R_Foot_y', 'L_Hip_y',\n",
       "       'L_Leg_y', 'L_Foot_y', 'Belly_y', 'Thorax_y', 'Neck_y', 'Head_y',\n",
       "       'L_Shoulder_y', 'L_Arm_y', 'L_Hand_y', 'R_Shoulder_y', 'R_Arm_y',\n",
       "       'R_Hand_y', 'Pelvis_z', 'R_Hip_z', 'R_Leg_z', 'R_Foot_z', 'L_Hip_z',\n",
       "       'L_Leg_z', 'L_Foot_z', 'Belly_z', 'Thorax_z', 'Neck_z', 'Head_z',\n",
       "       'L_Shoulder_z', 'L_Arm_z', 'L_Hand_z', 'R_Shoulder_z', 'R_Arm_z',\n",
       "       'R_Hand_z'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(feature_names.shape)\n",
    "print(features.shape)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boxing', 'Not Boxing']\n",
      "1\n",
      "Pelvis_x\n",
      "[ 122.     -63.588   17.062  188.94   310.96   409.72   371.24    98.658\n",
      "   99.22    84.662  100.32   388.79   481.29   433.59  -194.58  -406.78\n",
      " -269.24   153.08   122.05  -331.87  -418.68   195.62  -253.98  -422.21\n",
      "  186.26   132.12    39.25    76.775  186.46   219.64    58.649  124.93\n",
      "  164.91   -24.084  -34.884  -51.447   76.199 -172.48   -18.324  122.95\n",
      " -167.54    61.441  124.75   213.98   300.78   116.91   -84.036 -171.65\n",
      "   92.047 -419.57  -309.32 ]\n"
     ]
    }
   ],
   "source": [
    "# Look at our data\n",
    "print(label_names)\n",
    "print(labels[0])\n",
    "print(feature_names[0])\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data\n",
    "train, test, train_labels, test_labels = train_test_split(features,\n",
    "                                                          labels,\n",
    "                                                          test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39553307\n",
      "Iteration 2, loss = 0.23437865\n",
      "Iteration 3, loss = 0.19615568\n",
      "Iteration 4, loss = 0.18529257\n",
      "Iteration 5, loss = 0.17806187\n",
      "Iteration 6, loss = 0.17446527\n",
      "Iteration 7, loss = 0.17026339\n",
      "Iteration 8, loss = 0.16644692\n",
      "Iteration 9, loss = 0.16291283\n",
      "Iteration 10, loss = 0.16052583\n",
      "Iteration 11, loss = 0.15845580\n",
      "Iteration 12, loss = 0.15947562\n",
      "Iteration 13, loss = 0.15781464\n",
      "Iteration 14, loss = 0.15651258\n",
      "Iteration 15, loss = 0.15828915\n",
      "Iteration 16, loss = 0.15650158\n",
      "Iteration 17, loss = 0.15379696\n",
      "Iteration 18, loss = 0.15253121\n",
      "Iteration 19, loss = 0.15295822\n",
      "Iteration 20, loss = 0.15102759\n",
      "Iteration 21, loss = 0.14995518\n",
      "Iteration 22, loss = 0.14998710\n",
      "Iteration 23, loss = 0.14895484\n",
      "Iteration 24, loss = 0.14826343\n",
      "Iteration 25, loss = 0.14722951\n",
      "Iteration 26, loss = 0.14709617\n",
      "Iteration 27, loss = 0.14630517\n",
      "Iteration 28, loss = 0.14684979\n",
      "Iteration 29, loss = 0.14587811\n",
      "Iteration 30, loss = 0.14544758\n",
      "Iteration 31, loss = 0.14644859\n",
      "Iteration 32, loss = 0.14429779\n",
      "Iteration 33, loss = 0.14470612\n",
      "Iteration 34, loss = 0.14385240\n",
      "Iteration 35, loss = 0.14374626\n",
      "Iteration 36, loss = 0.14261111\n",
      "Iteration 37, loss = 0.14222651\n",
      "Iteration 38, loss = 0.14183528\n",
      "Iteration 39, loss = 0.14160764\n",
      "Iteration 40, loss = 0.14167146\n",
      "Iteration 41, loss = 0.14130334\n",
      "Iteration 42, loss = 0.14072178\n",
      "Iteration 43, loss = 0.14043564\n",
      "Iteration 44, loss = 0.14018342\n",
      "Iteration 45, loss = 0.13982115\n",
      "Iteration 46, loss = 0.13978296\n",
      "Iteration 47, loss = 0.13986863\n",
      "Iteration 48, loss = 0.13931920\n",
      "Iteration 49, loss = 0.13875103\n",
      "Iteration 50, loss = 0.13831106\n",
      "Iteration 51, loss = 0.13836738\n",
      "Iteration 52, loss = 0.13834113\n",
      "Iteration 53, loss = 0.13851844\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(40, 30, 20, 15, 10, 5, 2),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=200,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(40, 30, 20, 15, 10, 5, 2), learning_rate= 'adaptive', random_state=1, verbose=True)\n",
    "clf.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "preds = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'classifier_box_nonBox2.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17528,)\n",
      "(41437,)\n"
     ]
    }
   ],
   "source": [
    "np.sum(test_labels)/test_labels.shape[0]\n",
    "test_boxing = test_labels[np.where(test_labels==1)]\n",
    "test_nonboxing = test_labels[np.where(test_labels==0)]\n",
    "features_boxing = features[np.where(test_labels==1)][:]\n",
    "print(test_boxing.shape)\n",
    "print(test_nonboxing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.951818875604172\n",
      "Recall = 0.8975353719762665\n",
      "f1 score = 0.9171840839527766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluate accuracy\n",
    "print('Accuracy = {}'.format(accuracy_score(test_labels, preds)))\n",
    "# Evaluate precision\n",
    "#print('Precision = {}'.format(precision_score(test_labels, preds)))\n",
    "# Evaluate recall\n",
    "print('Recall = {}'.format(recall_score(test_labels, preds)))\n",
    "# Evaluate f1\n",
    "print('f1 score = {}'.format(f1_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40392  1045]\n",
      " [ 1796 15732]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(test_labels,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.96     41437\n",
      "          1       0.92      0.90      0.91     17528\n",
      "\n",
      "avg / total       0.95      0.95      0.95     58965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53041625\n",
      "Iteration 2, loss = 0.32276203\n",
      "Iteration 3, loss = 0.29094817\n",
      "Iteration 4, loss = 0.27553817\n",
      "Iteration 5, loss = 0.26431225\n",
      "Iteration 6, loss = 0.25591390\n",
      "Iteration 7, loss = 0.24828491\n",
      "Iteration 8, loss = 0.24297173\n",
      "Iteration 9, loss = 0.23926080\n",
      "Iteration 10, loss = 0.23512349\n",
      "Iteration 11, loss = 0.24215767\n",
      "Iteration 12, loss = 0.23232746\n",
      "Iteration 13, loss = 0.22751700\n",
      "Iteration 14, loss = 0.21986920\n",
      "Iteration 15, loss = 0.21220865\n",
      "Iteration 16, loss = 0.20866918\n",
      "Iteration 17, loss = 0.20478412\n",
      "Iteration 18, loss = 0.20480723\n",
      "Iteration 19, loss = 0.20266644\n",
      "Iteration 20, loss = 0.19947807\n",
      "Iteration 21, loss = 0.19955661\n",
      "Iteration 22, loss = 0.19441159\n",
      "Iteration 23, loss = 0.19279965\n",
      "Iteration 24, loss = 0.19131686\n",
      "Iteration 25, loss = 0.19114199\n",
      "Iteration 26, loss = 0.18904936\n",
      "Iteration 27, loss = 0.18746887\n",
      "Iteration 28, loss = 0.18690375\n",
      "Iteration 29, loss = 0.18529853\n",
      "Iteration 30, loss = 0.18375922\n",
      "Iteration 31, loss = 0.18228438\n",
      "Iteration 32, loss = 0.18118840\n",
      "Iteration 33, loss = 0.17991972\n",
      "Iteration 34, loss = 0.17896556\n",
      "Iteration 35, loss = 0.17871980\n",
      "Iteration 36, loss = 0.17800295\n",
      "Iteration 37, loss = 0.17599983\n",
      "Iteration 38, loss = 0.20335300\n",
      "Iteration 39, loss = 0.19009133\n",
      "Iteration 40, loss = 0.18592200\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33269276\n",
      "Iteration 2, loss = 0.25444522\n",
      "Iteration 3, loss = 0.23580238\n",
      "Iteration 4, loss = 0.22627584\n",
      "Iteration 5, loss = 0.21731530\n",
      "Iteration 6, loss = 0.21471942\n",
      "Iteration 7, loss = 0.20580321\n",
      "Iteration 8, loss = 0.19358857\n",
      "Iteration 9, loss = 0.18784089\n",
      "Iteration 10, loss = 0.18837442\n",
      "Iteration 11, loss = 0.18369609\n",
      "Iteration 12, loss = 0.18097085\n",
      "Iteration 13, loss = 0.17935431\n",
      "Iteration 14, loss = 0.17691775\n",
      "Iteration 15, loss = 0.17468626\n",
      "Iteration 16, loss = 0.17251596\n",
      "Iteration 17, loss = 0.17097629\n",
      "Iteration 18, loss = 0.17015996\n",
      "Iteration 19, loss = 0.16915526\n",
      "Iteration 20, loss = 0.16823660\n",
      "Iteration 21, loss = 0.16643851\n",
      "Iteration 22, loss = 0.16519073\n",
      "Iteration 23, loss = 0.16570860\n",
      "Iteration 24, loss = 0.16259000\n",
      "Iteration 25, loss = 0.16060843\n",
      "Iteration 26, loss = 0.15973369\n",
      "Iteration 27, loss = 0.15929937\n",
      "Iteration 28, loss = 0.16194459\n",
      "Iteration 29, loss = 0.15862525\n",
      "Iteration 30, loss = 0.15774397\n",
      "Iteration 31, loss = 0.15615745\n",
      "Iteration 32, loss = 0.15589807\n",
      "Iteration 33, loss = 0.15527498\n",
      "Iteration 34, loss = 0.15519597\n",
      "Iteration 35, loss = 0.15328636\n",
      "Iteration 36, loss = 0.15479647\n",
      "Iteration 37, loss = 0.15260585\n",
      "Iteration 38, loss = 0.15236079\n",
      "Iteration 39, loss = 0.15100716\n",
      "Iteration 40, loss = 0.15027259\n",
      "Iteration 41, loss = 0.15150770\n",
      "Iteration 42, loss = 0.14974057\n",
      "Iteration 43, loss = 0.14915083\n",
      "Iteration 44, loss = 0.14917723\n",
      "Iteration 45, loss = 0.14817093\n",
      "Iteration 46, loss = 0.14817309\n",
      "Iteration 47, loss = 0.14834051\n",
      "Iteration 48, loss = 0.14885211\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30844112\n",
      "Iteration 2, loss = 0.23919986\n",
      "Iteration 3, loss = 0.22044586\n",
      "Iteration 4, loss = 0.21317356\n",
      "Iteration 5, loss = 0.20729233\n",
      "Iteration 6, loss = 0.20358795\n",
      "Iteration 7, loss = 0.19757785\n",
      "Iteration 8, loss = 0.19220331\n",
      "Iteration 9, loss = 0.19040712\n",
      "Iteration 10, loss = 0.18977152\n",
      "Iteration 11, loss = 0.18617673\n",
      "Iteration 12, loss = 0.18409475\n",
      "Iteration 13, loss = 0.18205453\n",
      "Iteration 14, loss = 0.17967403\n",
      "Iteration 15, loss = 0.16826991\n",
      "Iteration 16, loss = 0.16308057\n",
      "Iteration 17, loss = 0.16152473\n",
      "Iteration 18, loss = 0.16051683\n",
      "Iteration 19, loss = 0.15855496\n",
      "Iteration 20, loss = 0.15692091\n",
      "Iteration 21, loss = 0.15597576\n",
      "Iteration 22, loss = 0.15510755\n",
      "Iteration 23, loss = 0.15586551\n",
      "Iteration 24, loss = 0.15480399\n",
      "Iteration 25, loss = 0.15267176\n",
      "Iteration 26, loss = 0.15282230\n",
      "Iteration 27, loss = 0.15152983\n",
      "Iteration 28, loss = 0.15202090\n",
      "Iteration 29, loss = 0.14924608\n",
      "Iteration 30, loss = 0.14893538\n",
      "Iteration 31, loss = 0.14812983\n",
      "Iteration 32, loss = 0.14739917\n",
      "Iteration 33, loss = 0.14681042\n",
      "Iteration 34, loss = 0.14784543\n",
      "Iteration 35, loss = 0.14595012\n",
      "Iteration 36, loss = 0.14610456\n",
      "Iteration 37, loss = 0.14424216\n",
      "Iteration 38, loss = 0.14421157\n",
      "Iteration 39, loss = 0.14381025\n",
      "Iteration 40, loss = 0.14552204\n",
      "Iteration 41, loss = 0.14377060\n",
      "Iteration 42, loss = 0.14303925\n",
      "Iteration 43, loss = 0.14237216\n",
      "Iteration 44, loss = 0.14361551\n",
      "Iteration 45, loss = 0.14153351\n",
      "Iteration 46, loss = 0.14172983\n",
      "Iteration 47, loss = 0.14110825\n",
      "Iteration 48, loss = 0.14072746\n",
      "Iteration 49, loss = 0.14119872\n",
      "Iteration 50, loss = 0.13992980\n",
      "Iteration 51, loss = 0.13981969\n",
      "Iteration 52, loss = 0.13899107\n",
      "Iteration 53, loss = 0.13941782\n",
      "Iteration 54, loss = 0.13862517\n",
      "Iteration 55, loss = 0.13848561\n",
      "Iteration 56, loss = 0.13826406\n",
      "Iteration 57, loss = 0.13803312\n",
      "Iteration 58, loss = 0.13746388\n",
      "Iteration 59, loss = 0.13772441\n",
      "Iteration 60, loss = 0.13672611\n",
      "Iteration 61, loss = 0.13678611\n",
      "Iteration 62, loss = 0.13645714\n",
      "Iteration 63, loss = 0.13628913\n",
      "Iteration 64, loss = 0.13584863\n",
      "Iteration 65, loss = 0.13635237\n",
      "Iteration 66, loss = 0.13614806\n",
      "Iteration 67, loss = 0.13492779\n",
      "Iteration 68, loss = 0.13554473\n",
      "Iteration 69, loss = 0.13475506\n",
      "Iteration 70, loss = 0.13479329\n",
      "Iteration 71, loss = 0.13506981\n",
      "Iteration 72, loss = 0.13435805\n",
      "Iteration 73, loss = 0.13411288\n",
      "Iteration 74, loss = 0.13436309\n",
      "Iteration 75, loss = 0.13410453\n",
      "Iteration 76, loss = 0.13440522\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29292229\n",
      "Iteration 2, loss = 0.22642398\n",
      "Iteration 3, loss = 0.21579098\n",
      "Iteration 4, loss = 0.20623130\n",
      "Iteration 5, loss = 0.19926167\n",
      "Iteration 6, loss = 0.19266664\n",
      "Iteration 7, loss = 0.18723077\n",
      "Iteration 8, loss = 0.18725817\n",
      "Iteration 9, loss = 0.18098328\n",
      "Iteration 10, loss = 0.17925694\n",
      "Iteration 11, loss = 0.17884473\n",
      "Iteration 12, loss = 0.17276235\n",
      "Iteration 13, loss = 0.16063602\n",
      "Iteration 14, loss = 0.15853667\n",
      "Iteration 15, loss = 0.16022949\n",
      "Iteration 16, loss = 0.15814923\n",
      "Iteration 17, loss = 0.15516594\n",
      "Iteration 18, loss = 0.15344435\n",
      "Iteration 19, loss = 0.15188054\n",
      "Iteration 20, loss = 0.15068202\n",
      "Iteration 21, loss = 0.15011544\n",
      "Iteration 22, loss = 0.14862802\n",
      "Iteration 23, loss = 0.14790057\n",
      "Iteration 24, loss = 0.14672691\n",
      "Iteration 25, loss = 0.14761418\n",
      "Iteration 26, loss = 0.14532577\n",
      "Iteration 27, loss = 0.14628385\n",
      "Iteration 28, loss = 0.14488914\n",
      "Iteration 29, loss = 0.14453869\n",
      "Iteration 30, loss = 0.14299210\n",
      "Iteration 31, loss = 0.14380857\n",
      "Iteration 32, loss = 0.14247704\n",
      "Iteration 33, loss = 0.14195775\n",
      "Iteration 34, loss = 0.14119613\n",
      "Iteration 35, loss = 0.14079524\n",
      "Iteration 36, loss = 0.14105633\n",
      "Iteration 37, loss = 0.13994663\n",
      "Iteration 38, loss = 0.14028998\n",
      "Iteration 39, loss = 0.14018419\n",
      "Iteration 40, loss = 0.13858695\n",
      "Iteration 41, loss = 0.13808625\n",
      "Iteration 42, loss = 0.13824742\n",
      "Iteration 43, loss = 0.13847679\n",
      "Iteration 44, loss = 0.13692431\n",
      "Iteration 45, loss = 0.13690108\n",
      "Iteration 46, loss = 0.13788904\n",
      "Iteration 47, loss = 0.13618003\n",
      "Iteration 48, loss = 0.13604856\n",
      "Iteration 49, loss = 0.13666143\n",
      "Iteration 50, loss = 0.13535924\n",
      "Iteration 51, loss = 0.13496919\n",
      "Iteration 52, loss = 0.13521097\n",
      "Iteration 53, loss = 0.13455854\n",
      "Iteration 54, loss = 0.13397871\n",
      "Iteration 55, loss = 0.13434919\n",
      "Iteration 56, loss = 0.13417400\n",
      "Iteration 57, loss = 0.13365624\n",
      "Iteration 58, loss = 0.13309694\n",
      "Iteration 59, loss = 0.13303299\n",
      "Iteration 60, loss = 0.13251292\n",
      "Iteration 61, loss = 0.13246156\n",
      "Iteration 62, loss = 0.13237346\n",
      "Iteration 63, loss = 0.13248668\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26939752\n",
      "Iteration 2, loss = 0.21907754\n",
      "Iteration 3, loss = 0.20676760\n",
      "Iteration 4, loss = 0.19765447\n",
      "Iteration 5, loss = 0.17801793\n",
      "Iteration 6, loss = 0.17386019\n",
      "Iteration 7, loss = 0.17537529\n",
      "Iteration 8, loss = 0.16768250\n",
      "Iteration 9, loss = 0.16422023\n",
      "Iteration 10, loss = 0.16088665\n",
      "Iteration 11, loss = 0.15946517\n",
      "Iteration 12, loss = 0.15659049\n",
      "Iteration 13, loss = 0.15541407\n",
      "Iteration 14, loss = 0.15313679\n",
      "Iteration 15, loss = 0.15178993\n",
      "Iteration 16, loss = 0.15111096\n",
      "Iteration 17, loss = 0.14948775\n",
      "Iteration 18, loss = 0.14890009\n",
      "Iteration 19, loss = 0.14774767\n",
      "Iteration 20, loss = 0.14712464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.14672461\n",
      "Iteration 22, loss = 0.14475549\n",
      "Iteration 23, loss = 0.14466879\n",
      "Iteration 24, loss = 0.14348484\n",
      "Iteration 25, loss = 0.14293767\n",
      "Iteration 26, loss = 0.14202187\n",
      "Iteration 27, loss = 0.14196076\n",
      "Iteration 28, loss = 0.14071513\n",
      "Iteration 29, loss = 0.14053712\n",
      "Iteration 30, loss = 0.14054619\n",
      "Iteration 31, loss = 0.13996065\n",
      "Iteration 32, loss = 0.14010322\n",
      "Iteration 33, loss = 0.13856622\n",
      "Iteration 34, loss = 0.13813228\n",
      "Iteration 35, loss = 0.13751578\n",
      "Iteration 36, loss = 0.13694942\n",
      "Iteration 37, loss = 0.13724371\n",
      "Iteration 38, loss = 0.13659958\n",
      "Iteration 39, loss = 0.13609733\n",
      "Iteration 40, loss = 0.13594433\n",
      "Iteration 41, loss = 0.13588600\n",
      "Iteration 42, loss = 0.13505134\n",
      "Iteration 43, loss = 0.13477674\n",
      "Iteration 44, loss = 0.13532775\n",
      "Iteration 45, loss = 0.13388400\n",
      "Iteration 46, loss = 0.13388548\n",
      "Iteration 47, loss = 0.13406401\n",
      "Iteration 48, loss = 0.13417465\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44057116\n",
      "Iteration 2, loss = 0.31423042\n",
      "Iteration 3, loss = 0.28311882\n",
      "Iteration 4, loss = 0.26459478\n",
      "Iteration 5, loss = 0.25464412\n",
      "Iteration 6, loss = 0.24387377\n",
      "Iteration 7, loss = 0.23885826\n",
      "Iteration 8, loss = 0.23227399\n",
      "Iteration 9, loss = 0.22708789\n",
      "Iteration 10, loss = 0.22238982\n",
      "Iteration 11, loss = 0.21874440\n",
      "Iteration 12, loss = 0.21622841\n",
      "Iteration 13, loss = 0.21895420\n",
      "Iteration 14, loss = 0.21208868\n",
      "Iteration 15, loss = 0.20867287\n",
      "Iteration 16, loss = 0.21544066\n",
      "Iteration 17, loss = 0.21215183\n",
      "Iteration 18, loss = 0.21275435\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34295731\n",
      "Iteration 2, loss = 0.26455882\n",
      "Iteration 3, loss = 0.24306893\n",
      "Iteration 4, loss = 0.23169998\n",
      "Iteration 5, loss = 0.22790996\n",
      "Iteration 6, loss = 0.20795995\n",
      "Iteration 7, loss = 0.20492679\n",
      "Iteration 8, loss = 0.19645376\n",
      "Iteration 9, loss = 0.19072876\n",
      "Iteration 10, loss = 0.18641365\n",
      "Iteration 11, loss = 0.18405860\n",
      "Iteration 12, loss = 0.18350838\n",
      "Iteration 13, loss = 0.18039735\n",
      "Iteration 14, loss = 0.17851121\n",
      "Iteration 15, loss = 0.17423100\n",
      "Iteration 16, loss = 0.17296664\n",
      "Iteration 17, loss = 0.17073195\n",
      "Iteration 18, loss = 0.16889446\n",
      "Iteration 19, loss = 0.16703583\n",
      "Iteration 20, loss = 0.16563996\n",
      "Iteration 21, loss = 0.16621836\n",
      "Iteration 22, loss = 0.16439204\n",
      "Iteration 23, loss = 0.16137772\n",
      "Iteration 24, loss = 0.16160271\n",
      "Iteration 25, loss = 0.15935563\n",
      "Iteration 26, loss = 0.15887317\n",
      "Iteration 27, loss = 0.15733116\n",
      "Iteration 28, loss = 0.16188313\n",
      "Iteration 29, loss = 0.15737340\n",
      "Iteration 30, loss = 0.15679807\n",
      "Iteration 31, loss = 0.15481948\n",
      "Iteration 32, loss = 0.15402347\n",
      "Iteration 33, loss = 0.15396650\n",
      "Iteration 34, loss = 0.15334450\n",
      "Iteration 35, loss = 0.15280715\n",
      "Iteration 36, loss = 0.15211470\n",
      "Iteration 37, loss = 0.15148052\n",
      "Iteration 38, loss = 0.15090282\n",
      "Iteration 39, loss = 0.15126473\n",
      "Iteration 40, loss = 0.15013065\n",
      "Iteration 41, loss = 0.14987425\n",
      "Iteration 42, loss = 0.14913113\n",
      "Iteration 43, loss = 0.14792650\n",
      "Iteration 44, loss = 0.14845924\n",
      "Iteration 45, loss = 0.14705792\n",
      "Iteration 46, loss = 0.14724565\n",
      "Iteration 47, loss = 0.14694499\n",
      "Iteration 48, loss = 0.14992754\n",
      "Iteration 49, loss = 0.14745435\n",
      "Iteration 50, loss = 0.14602655\n",
      "Iteration 51, loss = 0.14906135\n",
      "Iteration 52, loss = 0.14547842\n",
      "Iteration 53, loss = 0.14491960\n",
      "Iteration 54, loss = 0.14461753\n",
      "Iteration 55, loss = 0.14425724\n",
      "Iteration 56, loss = 0.14474155\n",
      "Iteration 57, loss = 0.14400034\n",
      "Iteration 58, loss = 0.14468151\n",
      "Iteration 59, loss = 0.14468402\n",
      "Iteration 60, loss = 0.14275809\n",
      "Iteration 61, loss = 0.14263070\n",
      "Iteration 62, loss = 0.14156581\n",
      "Iteration 63, loss = 0.14227408\n",
      "Iteration 64, loss = 0.14138668\n",
      "Iteration 65, loss = 0.14334232\n",
      "Iteration 66, loss = 0.14151481\n",
      "Iteration 67, loss = 0.14058536\n",
      "Iteration 68, loss = 0.14022797\n",
      "Iteration 69, loss = 0.13994435\n",
      "Iteration 70, loss = 0.14066825\n",
      "Iteration 71, loss = 0.14049692\n",
      "Iteration 72, loss = 0.14113771\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30339305\n",
      "Iteration 2, loss = 0.24618300\n",
      "Iteration 3, loss = 0.22012289\n",
      "Iteration 4, loss = 0.20413427\n",
      "Iteration 5, loss = 0.20091371\n",
      "Iteration 6, loss = 0.19295730\n",
      "Iteration 7, loss = 0.19524873\n",
      "Iteration 8, loss = 0.18451999\n",
      "Iteration 9, loss = 0.17988246\n",
      "Iteration 10, loss = 0.17668051\n",
      "Iteration 11, loss = 0.17463483\n",
      "Iteration 12, loss = 0.17082033\n",
      "Iteration 13, loss = 0.16948930\n",
      "Iteration 14, loss = 0.16678440\n",
      "Iteration 15, loss = 0.16547567\n",
      "Iteration 16, loss = 0.16495632\n",
      "Iteration 17, loss = 0.16195193\n",
      "Iteration 18, loss = 0.16232023\n",
      "Iteration 19, loss = 0.16032125\n",
      "Iteration 20, loss = 0.15954404\n",
      "Iteration 21, loss = 0.15835873\n",
      "Iteration 22, loss = 0.15648855\n",
      "Iteration 23, loss = 0.15763277\n",
      "Iteration 24, loss = 0.15573102\n",
      "Iteration 25, loss = 0.15563817\n",
      "Iteration 26, loss = 0.15336106\n",
      "Iteration 27, loss = 0.15375244\n",
      "Iteration 28, loss = 0.15233433\n",
      "Iteration 29, loss = 0.15187737\n",
      "Iteration 30, loss = 0.15098577\n",
      "Iteration 31, loss = 0.15056506\n",
      "Iteration 32, loss = 0.14930398\n",
      "Iteration 33, loss = 0.14846104\n",
      "Iteration 34, loss = 0.14980011\n",
      "Iteration 35, loss = 0.14706048\n",
      "Iteration 36, loss = 0.14722069\n",
      "Iteration 37, loss = 0.14865869\n",
      "Iteration 38, loss = 0.15111131\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29638130\n",
      "Iteration 2, loss = 0.23493349\n",
      "Iteration 3, loss = 0.21579951\n",
      "Iteration 4, loss = 0.20506895\n",
      "Iteration 5, loss = 0.19935032\n",
      "Iteration 6, loss = 0.19236669\n",
      "Iteration 7, loss = 0.18937946\n",
      "Iteration 8, loss = 0.18469209\n",
      "Iteration 9, loss = 0.18159091\n",
      "Iteration 10, loss = 0.17911799\n",
      "Iteration 11, loss = 0.17803326\n",
      "Iteration 12, loss = 0.17633804\n",
      "Iteration 13, loss = 0.17583067\n",
      "Iteration 14, loss = 0.17455104\n",
      "Iteration 15, loss = 0.17419205\n",
      "Iteration 16, loss = 0.17041143\n",
      "Iteration 17, loss = 0.16952892\n",
      "Iteration 18, loss = 0.16918787\n",
      "Iteration 19, loss = 0.16649529\n",
      "Iteration 20, loss = 0.16588154\n",
      "Iteration 21, loss = 0.16482310\n",
      "Iteration 22, loss = 0.16348691\n",
      "Iteration 23, loss = 0.16589400\n",
      "Iteration 24, loss = 0.16193021\n",
      "Iteration 25, loss = 0.16113303\n",
      "Iteration 26, loss = 0.16122620\n",
      "Iteration 27, loss = 0.15973044\n",
      "Iteration 28, loss = 0.16373529\n",
      "Iteration 29, loss = 0.15957783\n",
      "Iteration 30, loss = 0.15897756\n",
      "Iteration 31, loss = 0.15843379\n",
      "Iteration 32, loss = 0.15743315\n",
      "Iteration 33, loss = 0.15713347\n",
      "Iteration 34, loss = 0.15634760\n",
      "Iteration 35, loss = 0.15712684\n",
      "Iteration 36, loss = 0.15734908\n",
      "Iteration 37, loss = 0.15562935\n",
      "Iteration 38, loss = 0.15511118\n",
      "Iteration 39, loss = 0.15448686\n",
      "Iteration 40, loss = 0.15423096\n",
      "Iteration 41, loss = 0.15338833\n",
      "Iteration 42, loss = 0.15492116\n",
      "Iteration 43, loss = 0.15333445\n",
      "Iteration 44, loss = 0.15271270\n",
      "Iteration 45, loss = 0.15329014\n",
      "Iteration 46, loss = 0.15465818\n",
      "Iteration 47, loss = 0.15326811\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27155258\n",
      "Iteration 2, loss = 0.21947227\n",
      "Iteration 3, loss = 0.20393249\n",
      "Iteration 4, loss = 0.19519419\n",
      "Iteration 5, loss = 0.18074479\n",
      "Iteration 6, loss = 0.17177627\n",
      "Iteration 7, loss = 0.17265126\n",
      "Iteration 8, loss = 0.16679744\n",
      "Iteration 9, loss = 0.16298069\n",
      "Iteration 10, loss = 0.16032394\n",
      "Iteration 11, loss = 0.15828821\n",
      "Iteration 12, loss = 0.15663947\n",
      "Iteration 13, loss = 0.15444568\n",
      "Iteration 14, loss = 0.15339946\n",
      "Iteration 15, loss = 0.15132202\n",
      "Iteration 16, loss = 0.15209231\n",
      "Iteration 17, loss = 0.14915165\n",
      "Iteration 18, loss = 0.14834954\n",
      "Iteration 19, loss = 0.14739226\n",
      "Iteration 20, loss = 0.14634764\n",
      "Iteration 21, loss = 0.14601800\n",
      "Iteration 22, loss = 0.14482881\n",
      "Iteration 23, loss = 0.14484663\n",
      "Iteration 24, loss = 0.14409548\n",
      "Iteration 25, loss = 0.14444524\n",
      "Iteration 26, loss = 0.14269232\n",
      "Iteration 27, loss = 0.14191416\n",
      "Iteration 28, loss = 0.14171967\n",
      "Iteration 29, loss = 0.14149833\n",
      "Iteration 30, loss = 0.14036392\n",
      "Iteration 31, loss = 0.13971003\n",
      "Iteration 32, loss = 0.13967893\n",
      "Iteration 33, loss = 0.13889926\n",
      "Iteration 34, loss = 0.13882855\n",
      "Iteration 35, loss = 0.13824956\n",
      "Iteration 36, loss = 0.13785133\n",
      "Iteration 37, loss = 0.13779113\n",
      "Iteration 38, loss = 0.13708633\n",
      "Iteration 39, loss = 0.13769747\n",
      "Iteration 40, loss = 0.13750542\n",
      "Iteration 41, loss = 0.13650625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.13667279\n",
      "Iteration 43, loss = 0.13558704\n",
      "Iteration 44, loss = 0.13554388\n",
      "Iteration 45, loss = 0.13509543\n",
      "Iteration 46, loss = 0.13537234\n",
      "Iteration 47, loss = 0.13419602\n",
      "Iteration 48, loss = 0.13361654\n",
      "Iteration 49, loss = 0.13405646\n",
      "Iteration 50, loss = 0.13381697\n",
      "Iteration 51, loss = 0.13318453\n",
      "Iteration 52, loss = 0.13248674\n",
      "Iteration 53, loss = 0.13266358\n",
      "Iteration 54, loss = 0.13202332\n",
      "Iteration 55, loss = 0.13205913\n",
      "Iteration 56, loss = 0.13176157\n",
      "Iteration 57, loss = 0.13138649\n",
      "Iteration 58, loss = 0.13144359\n",
      "Iteration 59, loss = 0.13157510\n",
      "Iteration 60, loss = 0.13158674\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44057116\n",
      "Iteration 2, loss = 0.31423042\n",
      "Iteration 3, loss = 0.28311882\n",
      "Iteration 4, loss = 0.26459478\n",
      "Iteration 5, loss = 0.25464412\n",
      "Iteration 6, loss = 0.24387377\n",
      "Iteration 7, loss = 0.23885826\n",
      "Iteration 8, loss = 0.23227399\n",
      "Iteration 9, loss = 0.22708789\n",
      "Iteration 10, loss = 0.22238982\n",
      "Iteration 11, loss = 0.21874440\n",
      "Iteration 12, loss = 0.21622841\n",
      "Iteration 13, loss = 0.21895420\n",
      "Iteration 14, loss = 0.21208868\n",
      "Iteration 15, loss = 0.20867287\n",
      "Iteration 16, loss = 0.21544066\n",
      "Iteration 17, loss = 0.21215183\n",
      "Iteration 18, loss = 0.21275435\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34295731\n",
      "Iteration 2, loss = 0.26455882\n",
      "Iteration 3, loss = 0.24306893\n",
      "Iteration 4, loss = 0.23169998\n",
      "Iteration 5, loss = 0.22790996\n",
      "Iteration 6, loss = 0.20795995\n",
      "Iteration 7, loss = 0.20492679\n",
      "Iteration 8, loss = 0.19645376\n",
      "Iteration 9, loss = 0.19072876\n",
      "Iteration 10, loss = 0.18641365\n",
      "Iteration 11, loss = 0.18405860\n",
      "Iteration 12, loss = 0.18350838\n",
      "Iteration 13, loss = 0.18039735\n",
      "Iteration 14, loss = 0.17851121\n",
      "Iteration 15, loss = 0.17423100\n",
      "Iteration 16, loss = 0.17296664\n",
      "Iteration 17, loss = 0.17073195\n",
      "Iteration 18, loss = 0.16889446\n",
      "Iteration 19, loss = 0.16703583\n",
      "Iteration 20, loss = 0.16563996\n",
      "Iteration 21, loss = 0.16621836\n",
      "Iteration 22, loss = 0.16439204\n",
      "Iteration 23, loss = 0.16137772\n",
      "Iteration 24, loss = 0.16160271\n",
      "Iteration 25, loss = 0.15935563\n",
      "Iteration 26, loss = 0.15887317\n",
      "Iteration 27, loss = 0.15733116\n",
      "Iteration 28, loss = 0.16188313\n",
      "Iteration 29, loss = 0.15737340\n",
      "Iteration 30, loss = 0.15679807\n",
      "Iteration 31, loss = 0.15481948\n",
      "Iteration 32, loss = 0.15402347\n",
      "Iteration 33, loss = 0.15396650\n",
      "Iteration 34, loss = 0.15334450\n",
      "Iteration 35, loss = 0.15280715\n",
      "Iteration 36, loss = 0.15211470\n",
      "Iteration 37, loss = 0.15148052\n",
      "Iteration 38, loss = 0.15090282\n",
      "Iteration 39, loss = 0.15126473\n",
      "Iteration 40, loss = 0.15013065\n",
      "Iteration 41, loss = 0.14987425\n",
      "Iteration 42, loss = 0.14913113\n",
      "Iteration 43, loss = 0.14792650\n",
      "Iteration 44, loss = 0.14845924\n",
      "Iteration 45, loss = 0.14705792\n",
      "Iteration 46, loss = 0.14724565\n",
      "Iteration 47, loss = 0.14694499\n",
      "Iteration 48, loss = 0.14992754\n",
      "Iteration 49, loss = 0.14745435\n",
      "Iteration 50, loss = 0.14602655\n",
      "Iteration 51, loss = 0.14906135\n",
      "Iteration 52, loss = 0.14547842\n",
      "Iteration 53, loss = 0.14491960\n",
      "Iteration 54, loss = 0.14461753\n",
      "Iteration 55, loss = 0.14425724\n",
      "Iteration 56, loss = 0.14474155\n",
      "Iteration 57, loss = 0.14400034\n",
      "Iteration 58, loss = 0.14468151\n",
      "Iteration 59, loss = 0.14468402\n",
      "Iteration 60, loss = 0.14275809\n",
      "Iteration 61, loss = 0.14263070\n",
      "Iteration 62, loss = 0.14156581\n",
      "Iteration 63, loss = 0.14227408\n",
      "Iteration 64, loss = 0.14138668\n",
      "Iteration 65, loss = 0.14334232\n",
      "Iteration 66, loss = 0.14151481\n",
      "Iteration 67, loss = 0.14058536\n",
      "Iteration 68, loss = 0.14022797\n",
      "Iteration 69, loss = 0.13994435\n",
      "Iteration 70, loss = 0.14066825\n",
      "Iteration 71, loss = 0.14049692\n",
      "Iteration 72, loss = 0.14113771\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30329962\n",
      "Iteration 2, loss = 0.23527739\n",
      "Iteration 3, loss = 0.22188101\n",
      "Iteration 4, loss = 0.21030116\n",
      "Iteration 5, loss = 0.19162070\n",
      "Iteration 6, loss = 0.18448168\n",
      "Iteration 7, loss = 0.18404300\n",
      "Iteration 8, loss = 0.17850826\n",
      "Iteration 9, loss = 0.17754724\n",
      "Iteration 10, loss = 0.17408726\n",
      "Iteration 11, loss = 0.17045708\n",
      "Iteration 12, loss = 0.16855165\n",
      "Iteration 13, loss = 0.16707634\n",
      "Iteration 14, loss = 0.16651412\n",
      "Iteration 15, loss = 0.16393131\n",
      "Iteration 16, loss = 0.16395548\n",
      "Iteration 17, loss = 0.16294919\n",
      "Iteration 18, loss = 0.15974443\n",
      "Iteration 19, loss = 0.15769057\n",
      "Iteration 20, loss = 0.15902514\n",
      "Iteration 21, loss = 0.15613795\n",
      "Iteration 22, loss = 0.15580149\n",
      "Iteration 23, loss = 0.15488607\n",
      "Iteration 24, loss = 0.15465494\n",
      "Iteration 25, loss = 0.15292576\n",
      "Iteration 26, loss = 0.15138264\n",
      "Iteration 27, loss = 0.15159617\n",
      "Iteration 28, loss = 0.15125049\n",
      "Iteration 29, loss = 0.14937826\n",
      "Iteration 30, loss = 0.14882652\n",
      "Iteration 31, loss = 0.15022703\n",
      "Iteration 32, loss = 0.14714160\n",
      "Iteration 33, loss = 0.14675490\n",
      "Iteration 34, loss = 0.14789009\n",
      "Iteration 35, loss = 0.14583375\n",
      "Iteration 36, loss = 0.14523685\n",
      "Iteration 37, loss = 0.14512124\n",
      "Iteration 38, loss = 0.14536141\n",
      "Iteration 39, loss = 0.14504684\n",
      "Iteration 40, loss = 0.14316227\n",
      "Iteration 41, loss = 0.14301252\n",
      "Iteration 42, loss = 0.14337977\n",
      "Iteration 43, loss = 0.14284845\n",
      "Iteration 44, loss = 0.14304112\n",
      "Iteration 45, loss = 0.14320676\n",
      "Iteration 46, loss = 0.14168130\n",
      "Iteration 47, loss = 0.14145298\n",
      "Iteration 48, loss = 0.14144541\n",
      "Iteration 49, loss = 0.14039456\n",
      "Iteration 50, loss = 0.14127738\n",
      "Iteration 51, loss = 0.14011730\n",
      "Iteration 52, loss = 0.14030449\n",
      "Iteration 53, loss = 0.13918146\n",
      "Iteration 54, loss = 0.14325895\n",
      "Iteration 55, loss = 0.14026257\n",
      "Iteration 56, loss = 0.13887767\n",
      "Iteration 57, loss = 0.13827400\n",
      "Iteration 58, loss = 0.13769613\n",
      "Iteration 59, loss = 0.13782228\n",
      "Iteration 60, loss = 0.13930641\n",
      "Iteration 61, loss = 0.13665417\n",
      "Iteration 62, loss = 0.13633005\n",
      "Iteration 63, loss = 0.13630968\n",
      "Iteration 64, loss = 0.13833367\n",
      "Iteration 65, loss = 0.13852170\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29040545\n",
      "Iteration 2, loss = 0.22395995\n",
      "Iteration 3, loss = 0.21007638\n",
      "Iteration 4, loss = 0.20291537\n",
      "Iteration 5, loss = 0.19498246\n",
      "Iteration 6, loss = 0.19645971\n",
      "Iteration 7, loss = 0.18756401\n",
      "Iteration 8, loss = 0.18550022\n",
      "Iteration 9, loss = 0.18387260\n",
      "Iteration 10, loss = 0.17981735\n",
      "Iteration 11, loss = 0.18029963\n",
      "Iteration 12, loss = 0.17662456\n",
      "Iteration 13, loss = 0.17295958\n",
      "Iteration 14, loss = 0.16152179\n",
      "Iteration 15, loss = 0.15725614\n",
      "Iteration 16, loss = 0.15694047\n",
      "Iteration 17, loss = 0.15458571\n",
      "Iteration 18, loss = 0.15309848\n",
      "Iteration 19, loss = 0.15300914\n",
      "Iteration 20, loss = 0.15230280\n",
      "Iteration 21, loss = 0.15249406\n",
      "Iteration 22, loss = 0.15048607\n",
      "Iteration 23, loss = 0.14833500\n",
      "Iteration 24, loss = 0.14769112\n",
      "Iteration 25, loss = 0.14729849\n",
      "Iteration 26, loss = 0.14826816\n",
      "Iteration 27, loss = 0.14735973\n",
      "Iteration 28, loss = 0.14581143\n",
      "Iteration 29, loss = 0.14480692\n",
      "Iteration 30, loss = 0.14419391\n",
      "Iteration 31, loss = 0.14347865\n",
      "Iteration 32, loss = 0.14289963\n",
      "Iteration 33, loss = 0.14231542\n",
      "Iteration 34, loss = 0.14266072\n",
      "Iteration 35, loss = 0.14158065\n",
      "Iteration 36, loss = 0.14150253\n",
      "Iteration 37, loss = 0.14069688\n",
      "Iteration 38, loss = 0.14054339\n",
      "Iteration 39, loss = 0.14027630\n",
      "Iteration 40, loss = 0.14072833\n",
      "Iteration 41, loss = 0.13949738\n",
      "Iteration 42, loss = 0.14368174\n",
      "Iteration 43, loss = 0.14094455\n",
      "Iteration 44, loss = 0.13917314\n",
      "Iteration 45, loss = 0.13870075\n",
      "Iteration 46, loss = 0.13892657\n",
      "Iteration 47, loss = 0.13981850\n",
      "Iteration 48, loss = 0.13885547\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27491002\n",
      "Iteration 2, loss = 0.21652793\n",
      "Iteration 3, loss = 0.20269258\n",
      "Iteration 4, loss = 0.18100011\n",
      "Iteration 5, loss = 0.17539652\n",
      "Iteration 6, loss = 0.17084410\n",
      "Iteration 7, loss = 0.16693914\n",
      "Iteration 8, loss = 0.16324689\n",
      "Iteration 9, loss = 0.16086407\n",
      "Iteration 10, loss = 0.15848182\n",
      "Iteration 11, loss = 0.15637708\n",
      "Iteration 12, loss = 0.15606858\n",
      "Iteration 13, loss = 0.15359675\n",
      "Iteration 14, loss = 0.15245824\n",
      "Iteration 15, loss = 0.15084449\n",
      "Iteration 16, loss = 0.15024999\n",
      "Iteration 17, loss = 0.14919713\n",
      "Iteration 18, loss = 0.14857134\n",
      "Iteration 19, loss = 0.14736659\n",
      "Iteration 20, loss = 0.14637423\n",
      "Iteration 21, loss = 0.14586442\n",
      "Iteration 22, loss = 0.14561796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.14402080\n",
      "Iteration 24, loss = 0.14406493\n",
      "Iteration 25, loss = 0.14576256\n",
      "Iteration 26, loss = 0.14221598\n",
      "Iteration 27, loss = 0.14185273\n",
      "Iteration 28, loss = 0.14252615\n",
      "Iteration 29, loss = 0.14109228\n",
      "Iteration 30, loss = 0.14074714\n",
      "Iteration 31, loss = 0.14069352\n",
      "Iteration 32, loss = 0.13989409\n",
      "Iteration 33, loss = 0.13983687\n",
      "Iteration 34, loss = 0.13913781\n",
      "Iteration 35, loss = 0.13902212\n",
      "Iteration 36, loss = 0.13920844\n",
      "Iteration 37, loss = 0.13825105\n",
      "Iteration 38, loss = 0.13727984\n",
      "Iteration 39, loss = 0.13734571\n",
      "Iteration 40, loss = 0.13809591\n",
      "Iteration 41, loss = 0.13716836\n",
      "Iteration 42, loss = 0.13616879\n",
      "Iteration 43, loss = 0.13590111\n",
      "Iteration 44, loss = 0.13604078\n",
      "Iteration 45, loss = 0.13557647\n",
      "Iteration 46, loss = 0.13531926\n",
      "Iteration 47, loss = 0.13512785\n",
      "Iteration 48, loss = 0.13467087\n",
      "Iteration 49, loss = 0.13438947\n",
      "Iteration 50, loss = 0.13377485\n",
      "Iteration 51, loss = 0.13367593\n",
      "Iteration 52, loss = 0.13334059\n",
      "Iteration 53, loss = 0.13323813\n",
      "Iteration 54, loss = 0.13320877\n",
      "Iteration 55, loss = 0.13547653\n",
      "Iteration 56, loss = 0.13257786\n",
      "Iteration 57, loss = 0.13302784\n",
      "Iteration 58, loss = 0.13242340\n",
      "Iteration 59, loss = 0.13289551\n",
      "Iteration 60, loss = 0.13248721\n",
      "Iteration 61, loss = 0.13142951\n",
      "Iteration 62, loss = 0.13189399\n",
      "Iteration 63, loss = 0.13103141\n",
      "Iteration 64, loss = 0.13106332\n",
      "Iteration 65, loss = 0.13084624\n",
      "Iteration 66, loss = 0.13276414\n",
      "Iteration 67, loss = 0.13021301\n",
      "Iteration 68, loss = 0.13019939\n",
      "Iteration 69, loss = 0.13028591\n",
      "Iteration 70, loss = 0.13017783\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18521481b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x185200dd7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot Learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import learning_curve\n",
    "plt.figure()\n",
    "plt.title('Learning')\n",
    "ylim = None\n",
    "if ylim is not None:\n",
    "    plt.ylim(*ylim)\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, train, train_labels, cv=None, n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3de001cc2de6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
