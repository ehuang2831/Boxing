{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "features = pd.read_csv('../Data/FormattedTraining/ShadowVideosPhotosAugmented/data.csv')\n",
    "labels = pd.read_csv('../Data/FormattedTraining/ShadowVideosPhotosAugmented/target.csv')\n",
    "# Even spread of boxing vs non boxing (1-2)\n",
    "#features = pd.read_csv('../Data/FormattedTraining/ShadowVideos/data_even.csv')\n",
    "#labels = pd.read_csv('../Data/FormattedTraining/ShadowVideos/target_even.csv')\n",
    "feature_names = pd.read_csv('../Data/FormattedTraining/ShadowVideosPhotosAugmented/feature_names.csv')\n",
    "label_names = ['Boxing','Not Boxing']\n",
    "\n",
    "labels = np.ravel(labels.as_matrix(columns=None))\n",
    "feature_names = np.ravel(feature_names.as_matrix(columns=None))\n",
    "features = features.as_matrix(columns=None)\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095171,)\n",
      "(51,)\n",
      "(1095171, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Pelvis_x', 'R_Hip_x', 'R_Leg_x', 'R_Foot_x', 'L_Hip_x', 'L_Leg_x',\n",
       "       'L_Foot_x', 'Belly_x', 'Thorax_x', 'Neck_x', 'Head_x',\n",
       "       'L_Shoulder_x', 'L_Arm_x', 'L_Hand_x', 'R_Shoulder_x', 'R_Arm_x',\n",
       "       'R_Hand_x', 'Pelvis_y', 'R_Hip_y', 'R_Leg_y', 'R_Foot_y', 'L_Hip_y',\n",
       "       'L_Leg_y', 'L_Foot_y', 'Belly_y', 'Thorax_y', 'Neck_y', 'Head_y',\n",
       "       'L_Shoulder_y', 'L_Arm_y', 'L_Hand_y', 'R_Shoulder_y', 'R_Arm_y',\n",
       "       'R_Hand_y', 'Pelvis_z', 'R_Hip_z', 'R_Leg_z', 'R_Foot_z', 'L_Hip_z',\n",
       "       'L_Leg_z', 'L_Foot_z', 'Belly_z', 'Thorax_z', 'Neck_z', 'Head_z',\n",
       "       'L_Shoulder_z', 'L_Arm_z', 'L_Hand_z', 'R_Shoulder_z', 'R_Arm_z',\n",
       "       'R_Hand_z'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(feature_names.shape)\n",
    "print(features.shape)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boxing', 'Not Boxing']\n",
      "0\n",
      "Pelvis_x\n",
      "[ 469.4     329.6     460.3     703.64    609.2     718.29    775.51\n",
      "  416.04    400.38    380.45    432.61    614.13    656.03    744.33\n",
      "  157.21    -86.01   -297.37     98.073    40.028  -282.9    -479.82\n",
      "  156.12   -185.05   -454.19    184.45    183.83    117.73    142.4     242.11\n",
      "  390.      335.39    169.29    364.46    132.93    -20.84    -48.578\n",
      "  244.92    189.59      6.8984  302.19    187.15      5.0915  -28.924\n",
      "  104.62    302.77     -1.2804 -134.62   -319.4    -105.78   -498.5    -596.74  ]\n"
     ]
    }
   ],
   "source": [
    "# Look at our data\n",
    "print(label_names)\n",
    "print(labels[0])\n",
    "print(feature_names[0])\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data\n",
    "train, test, train_labels, test_labels = train_test_split(features,\n",
    "                                                          labels,\n",
    "                                                          test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.23611936\n",
      "Iteration 2, loss = 0.18606762\n",
      "Iteration 3, loss = 0.17078826\n",
      "Iteration 4, loss = 0.16201246\n",
      "Iteration 5, loss = 0.15478867\n",
      "Iteration 6, loss = 0.15002234\n",
      "Iteration 7, loss = 0.14622850\n",
      "Iteration 8, loss = 0.14325546\n",
      "Iteration 9, loss = 0.14010270\n",
      "Iteration 10, loss = 0.13802041\n",
      "Iteration 11, loss = 0.13579457\n",
      "Iteration 12, loss = 0.13504552\n",
      "Iteration 13, loss = 0.13268278\n",
      "Iteration 14, loss = 0.13117150\n",
      "Iteration 15, loss = 0.13140797\n",
      "Iteration 16, loss = 0.12925088\n",
      "Iteration 17, loss = 0.12830261\n",
      "Iteration 18, loss = 0.12737117\n",
      "Iteration 19, loss = 0.12644747\n",
      "Iteration 20, loss = 0.12612768\n",
      "Iteration 21, loss = 0.12492312\n",
      "Iteration 22, loss = 0.12406356\n",
      "Iteration 23, loss = 0.12401357\n",
      "Iteration 24, loss = 0.12347690\n",
      "Iteration 25, loss = 0.12282679\n",
      "Iteration 26, loss = 0.12228960\n",
      "Iteration 27, loss = 0.12164634\n",
      "Iteration 28, loss = 0.12158245\n",
      "Iteration 29, loss = 0.11973495\n",
      "Iteration 30, loss = 0.11996442\n",
      "Iteration 31, loss = 0.12018413\n",
      "Iteration 32, loss = 0.11893994\n",
      "Iteration 33, loss = 0.11857868\n",
      "Iteration 34, loss = 0.11955518\n",
      "Iteration 35, loss = 0.11828823\n",
      "Iteration 36, loss = 0.11795069\n",
      "Iteration 37, loss = 0.11723778\n",
      "Iteration 38, loss = 0.11773081\n",
      "Iteration 39, loss = 0.11647077\n",
      "Iteration 40, loss = 0.11648137\n",
      "Iteration 41, loss = 0.11685749\n",
      "Iteration 42, loss = 0.11597421\n",
      "Iteration 43, loss = 0.11533788\n",
      "Iteration 44, loss = 0.11487407\n",
      "Iteration 45, loss = 0.11470871\n",
      "Iteration 46, loss = 0.11439606\n",
      "Iteration 47, loss = 0.11484603\n",
      "Iteration 48, loss = 0.11394511\n",
      "Iteration 49, loss = 0.11355675\n",
      "Iteration 50, loss = 0.11365938\n",
      "Iteration 51, loss = 0.11405144\n",
      "Iteration 52, loss = 0.11306334\n",
      "Iteration 53, loss = 0.11307962\n",
      "Iteration 54, loss = 0.11336451\n",
      "Iteration 55, loss = 0.11310171\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(40, 30, 20, 10, 5, 2), learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='adam', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(40, 30, 20, 10, 5, 2), learning_rate= 'adaptive', random_state=1, verbose=True)\n",
    "clf.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "preds = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'classifier_box_nonBox.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open('classifier_box_nonBox2.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new dataset\n",
    "newData = pd.read_csv('../Data/FormattedVideos/Labelled_to_classify/data.csv')\n",
    "newPred = loaded_model.predict(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(853217,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print back into observable\n",
    "#newData.shape\n",
    "#newPred.reshape((-1,1)).shape\n",
    "df = pd.DataFrame(np.concatenate((newPred.reshape((-1,1)),newData),axis=1))\n",
    "df.to_csv('labelledVideos2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2827,)\n",
      "(8125,)\n"
     ]
    }
   ],
   "source": [
    "np.sum(test_labels)/test_labels.shape[0]\n",
    "test_boxing = test_labels[np.where(test_labels==1)]\n",
    "test_nonboxing = test_labels[np.where(test_labels==0)]\n",
    "features_boxing = features[np.where(test_labels==1)][:]\n",
    "print(test_boxing.shape)\n",
    "print(test_nonboxing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9602805018353148\n",
      "Recall = 0.8841394025604552\n",
      "f1 score = 0.9195546843214853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluate accuracy\n",
    "print('Accuracy = {}'.format(accuracy_score(test_labels, preds)))\n",
    "# Evaluate precision\n",
    "#print('Precision = {}'.format(precision_score(test_labels, preds)))\n",
    "# Evaluate recall\n",
    "print('Recall = {}'.format(recall_score(test_labels, preds)))\n",
    "# Evaluate f1\n",
    "print('f1 score = {}'.format(f1_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8032   93]\n",
      " [ 317 2510]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(test_labels,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97     40699\n",
      "          1       0.96      0.88      0.92     14060\n",
      "\n",
      "avg / total       0.96      0.96      0.96     54759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedric.fraces\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:811: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(train_sizes_abs.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.44250207\n",
      "Iteration 2, loss = 0.30281346\n",
      "Iteration 3, loss = 0.26692015\n",
      "Iteration 4, loss = 0.25049286\n",
      "Iteration 5, loss = 0.23949508\n",
      "Iteration 6, loss = 0.22773941\n",
      "Iteration 7, loss = 0.21960764\n",
      "Iteration 8, loss = 0.21410257\n",
      "Iteration 9, loss = 0.21960981\n",
      "Iteration 10, loss = 0.21026964\n",
      "Iteration 11, loss = 0.20520695\n",
      "Iteration 12, loss = 0.20048797\n",
      "Iteration 13, loss = 0.19676793\n",
      "Iteration 14, loss = 0.19476825\n",
      "Iteration 15, loss = 0.19132794\n",
      "Iteration 16, loss = 0.18994791\n",
      "Iteration 17, loss = 0.18755509\n",
      "Iteration 18, loss = 0.18528370\n",
      "Iteration 19, loss = 0.18242494\n",
      "Iteration 20, loss = 0.18093333\n",
      "Iteration 21, loss = 0.17828195\n",
      "Iteration 22, loss = 0.17758975\n",
      "Iteration 23, loss = 0.19706019\n",
      "Iteration 24, loss = 0.18543553\n",
      "Iteration 25, loss = 0.17982814\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32294869\n",
      "Iteration 2, loss = 0.24059220\n",
      "Iteration 3, loss = 0.22138813\n",
      "Iteration 4, loss = 0.20921682\n",
      "Iteration 5, loss = 0.19777221\n",
      "Iteration 6, loss = 0.19659800\n",
      "Iteration 7, loss = 0.18757047\n",
      "Iteration 8, loss = 0.18206933\n",
      "Iteration 9, loss = 0.17811035\n",
      "Iteration 10, loss = 0.18013010\n",
      "Iteration 11, loss = 0.17909743\n",
      "Iteration 12, loss = 0.17201578\n",
      "Iteration 13, loss = 0.16770884\n",
      "Iteration 14, loss = 0.16922142\n",
      "Iteration 15, loss = 0.16639997\n",
      "Iteration 16, loss = 0.16332430\n",
      "Iteration 17, loss = 0.16182666\n",
      "Iteration 18, loss = 0.16247342\n",
      "Iteration 19, loss = 0.15905807\n",
      "Iteration 20, loss = 0.15686100\n",
      "Iteration 21, loss = 0.16106452\n",
      "Iteration 22, loss = 0.16352506\n",
      "Iteration 23, loss = 0.15620473\n",
      "Iteration 24, loss = 0.15533423\n",
      "Iteration 25, loss = 0.15291032\n",
      "Iteration 26, loss = 0.15103755\n",
      "Iteration 27, loss = 0.15011101\n",
      "Iteration 28, loss = 0.14947791\n",
      "Iteration 29, loss = 0.15042693\n",
      "Iteration 30, loss = 0.14797779\n",
      "Iteration 31, loss = 0.14593806\n",
      "Iteration 32, loss = 0.14612923\n",
      "Iteration 33, loss = 0.14505324\n",
      "Iteration 34, loss = 0.14417947\n",
      "Iteration 35, loss = 0.14605343\n",
      "Iteration 36, loss = 0.14252746\n",
      "Iteration 37, loss = 0.14284915\n",
      "Iteration 38, loss = 0.14144806\n",
      "Iteration 39, loss = 0.14097345\n",
      "Iteration 40, loss = 0.14022722\n",
      "Iteration 41, loss = 0.13964272\n",
      "Iteration 42, loss = 0.13897785\n",
      "Iteration 43, loss = 0.14043184\n",
      "Iteration 44, loss = 0.13814772\n",
      "Iteration 45, loss = 0.13741424\n",
      "Iteration 46, loss = 0.13938914\n",
      "Iteration 47, loss = 0.13652593\n",
      "Iteration 48, loss = 0.13599787\n",
      "Iteration 49, loss = 0.13496418\n",
      "Iteration 50, loss = 0.13531758\n",
      "Iteration 51, loss = 0.13408534\n",
      "Iteration 52, loss = 0.13439737\n",
      "Iteration 53, loss = 0.13310153\n",
      "Iteration 54, loss = 0.13296203\n",
      "Iteration 55, loss = 0.13283885\n",
      "Iteration 56, loss = 0.13205777\n",
      "Iteration 57, loss = 0.13196294\n",
      "Iteration 58, loss = 0.13239842\n",
      "Iteration 59, loss = 0.13197099\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29595340\n",
      "Iteration 2, loss = 0.22958989\n",
      "Iteration 3, loss = 0.21146452\n",
      "Iteration 4, loss = 0.19790151\n",
      "Iteration 5, loss = 0.18612928\n",
      "Iteration 6, loss = 0.18037237\n",
      "Iteration 7, loss = 0.17387065\n",
      "Iteration 8, loss = 0.16972587\n",
      "Iteration 9, loss = 0.16783850\n",
      "Iteration 10, loss = 0.16212949\n",
      "Iteration 11, loss = 0.15923216\n",
      "Iteration 12, loss = 0.15858445\n",
      "Iteration 13, loss = 0.15563718\n",
      "Iteration 14, loss = 0.15234060\n",
      "Iteration 15, loss = 0.15063242\n",
      "Iteration 16, loss = 0.15178541\n",
      "Iteration 17, loss = 0.14827231\n",
      "Iteration 18, loss = 0.14693117\n",
      "Iteration 19, loss = 0.14654842\n",
      "Iteration 20, loss = 0.14764076\n",
      "Iteration 21, loss = 0.14411937\n",
      "Iteration 22, loss = 0.14654370\n",
      "Iteration 23, loss = 0.14637869\n",
      "Iteration 24, loss = 0.14265848\n",
      "Iteration 25, loss = 0.14206908\n",
      "Iteration 26, loss = 0.14134333\n",
      "Iteration 27, loss = 0.14026406\n",
      "Iteration 28, loss = 0.13958085\n",
      "Iteration 29, loss = 0.13817350\n",
      "Iteration 30, loss = 0.13724923\n",
      "Iteration 31, loss = 0.13770016\n",
      "Iteration 32, loss = 0.13520524\n",
      "Iteration 33, loss = 0.13465277\n",
      "Iteration 34, loss = 0.13492010\n",
      "Iteration 35, loss = 0.13507438\n",
      "Iteration 36, loss = 0.13384491\n",
      "Iteration 37, loss = 0.13281609\n",
      "Iteration 38, loss = 0.13201369\n",
      "Iteration 39, loss = 0.13206391\n",
      "Iteration 40, loss = 0.13121080\n",
      "Iteration 41, loss = 0.13079427\n",
      "Iteration 42, loss = 0.12992092\n",
      "Iteration 43, loss = 0.12984120\n",
      "Iteration 44, loss = 0.12904260\n",
      "Iteration 45, loss = 0.12933053\n",
      "Iteration 46, loss = 0.12805140\n",
      "Iteration 47, loss = 0.12830936\n",
      "Iteration 48, loss = 0.12739924\n",
      "Iteration 49, loss = 0.12773545\n",
      "Iteration 50, loss = 0.12726608\n",
      "Iteration 51, loss = 0.12684072\n",
      "Iteration 52, loss = 0.12606367\n",
      "Iteration 53, loss = 0.12597497\n",
      "Iteration 54, loss = 0.13090186\n",
      "Iteration 55, loss = 0.12587527\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27189460\n",
      "Iteration 2, loss = 0.20427682\n",
      "Iteration 3, loss = 0.18975990\n",
      "Iteration 4, loss = 0.17865501\n",
      "Iteration 5, loss = 0.17281496\n",
      "Iteration 6, loss = 0.16840305\n",
      "Iteration 7, loss = 0.16370615\n",
      "Iteration 8, loss = 0.16327230\n",
      "Iteration 9, loss = 0.15777403\n",
      "Iteration 10, loss = 0.15511799\n",
      "Iteration 11, loss = 0.15519794\n",
      "Iteration 12, loss = 0.15094299\n",
      "Iteration 13, loss = 0.15369443\n",
      "Iteration 14, loss = 0.14839961\n",
      "Iteration 15, loss = 0.14767554\n",
      "Iteration 16, loss = 0.14534591\n",
      "Iteration 17, loss = 0.14382570\n",
      "Iteration 18, loss = 0.14252713\n",
      "Iteration 19, loss = 0.14127753\n",
      "Iteration 20, loss = 0.13974555\n",
      "Iteration 21, loss = 0.14017362\n",
      "Iteration 22, loss = 0.13773072\n",
      "Iteration 23, loss = 0.13670384\n",
      "Iteration 24, loss = 0.13704764\n",
      "Iteration 25, loss = 0.13593180\n",
      "Iteration 26, loss = 0.13422483\n",
      "Iteration 27, loss = 0.13458266\n",
      "Iteration 28, loss = 0.13297941\n",
      "Iteration 29, loss = 0.13284749\n",
      "Iteration 30, loss = 0.13196205\n",
      "Iteration 31, loss = 0.13338374\n",
      "Iteration 32, loss = 0.13121973\n",
      "Iteration 33, loss = 0.13246850\n",
      "Iteration 34, loss = 0.12979764\n",
      "Iteration 35, loss = 0.12917973\n",
      "Iteration 36, loss = 0.12949716\n",
      "Iteration 37, loss = 0.12863261\n",
      "Iteration 38, loss = 0.12835008\n",
      "Iteration 39, loss = 0.12855569\n",
      "Iteration 40, loss = 0.12726356\n",
      "Iteration 41, loss = 0.12726215\n",
      "Iteration 42, loss = 0.12698679\n",
      "Iteration 43, loss = 0.12683908\n",
      "Iteration 44, loss = 0.12688915\n",
      "Iteration 45, loss = 0.12609444\n",
      "Iteration 46, loss = 0.12527176\n",
      "Iteration 47, loss = 0.12555023\n",
      "Iteration 48, loss = 0.12466636\n",
      "Iteration 49, loss = 0.12500003\n",
      "Iteration 50, loss = 0.12519180\n",
      "Iteration 51, loss = 0.12332376\n",
      "Iteration 52, loss = 0.12350178\n",
      "Iteration 53, loss = 0.12325131\n",
      "Iteration 54, loss = 0.12463955\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26895804\n",
      "Iteration 2, loss = 0.20904556\n",
      "Iteration 3, loss = 0.18896233\n",
      "Iteration 4, loss = 0.17657208\n",
      "Iteration 5, loss = 0.17023504\n",
      "Iteration 6, loss = 0.16384089\n",
      "Iteration 7, loss = 0.15976918\n",
      "Iteration 8, loss = 0.15570857\n",
      "Iteration 9, loss = 0.15262287\n",
      "Iteration 10, loss = 0.15044244\n",
      "Iteration 11, loss = 0.14762160\n",
      "Iteration 12, loss = 0.14557236\n",
      "Iteration 13, loss = 0.14352896\n",
      "Iteration 14, loss = 0.14189108\n",
      "Iteration 15, loss = 0.14077534\n",
      "Iteration 16, loss = 0.14144945\n",
      "Iteration 17, loss = 0.13904305\n",
      "Iteration 18, loss = 0.13687452\n",
      "Iteration 19, loss = 0.13623102\n",
      "Iteration 20, loss = 0.13636754\n",
      "Iteration 21, loss = 0.13411509\n",
      "Iteration 22, loss = 0.13308149\n",
      "Iteration 23, loss = 0.13262288\n",
      "Iteration 24, loss = 0.13147062\n",
      "Iteration 25, loss = 0.13051069\n",
      "Iteration 26, loss = 0.12997294\n",
      "Iteration 27, loss = 0.12940070\n",
      "Iteration 28, loss = 0.12893335\n",
      "Iteration 29, loss = 0.12869259\n",
      "Iteration 30, loss = 0.12751244\n",
      "Iteration 31, loss = 0.12735312\n",
      "Iteration 32, loss = 0.12693361\n",
      "Iteration 33, loss = 0.12622054\n",
      "Iteration 34, loss = 0.12583217\n",
      "Iteration 35, loss = 0.12553110\n",
      "Iteration 36, loss = 0.12558977\n",
      "Iteration 37, loss = 0.12459172\n",
      "Iteration 38, loss = 0.12433452\n",
      "Iteration 39, loss = 0.12448075\n",
      "Iteration 40, loss = 0.12420056\n",
      "Iteration 41, loss = 0.12298988\n",
      "Iteration 42, loss = 0.12323940\n",
      "Iteration 43, loss = 0.12263748\n",
      "Iteration 44, loss = 0.12201105\n",
      "Iteration 45, loss = 0.12181628\n",
      "Iteration 46, loss = 0.12119678\n",
      "Iteration 47, loss = 0.12137835\n",
      "Iteration 48, loss = 0.12080916\n",
      "Iteration 49, loss = 0.12069275\n",
      "Iteration 50, loss = 0.12071061\n",
      "Iteration 51, loss = 0.11970402\n",
      "Iteration 52, loss = 0.11986835\n",
      "Iteration 53, loss = 0.11930039\n",
      "Iteration 54, loss = 0.11924546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.11893964\n",
      "Iteration 56, loss = 0.11872746\n",
      "Iteration 57, loss = 0.11860219\n",
      "Iteration 58, loss = 0.11818561\n",
      "Iteration 59, loss = 0.11801843\n",
      "Iteration 60, loss = 0.11759507\n",
      "Iteration 61, loss = 0.11717239\n",
      "Iteration 62, loss = 0.11753666\n",
      "Iteration 63, loss = 0.11747122\n",
      "Iteration 64, loss = 0.11690101\n",
      "Iteration 65, loss = 0.11698200\n",
      "Iteration 66, loss = 0.11679293\n",
      "Iteration 67, loss = 0.11769778\n",
      "Iteration 68, loss = 0.11820525\n",
      "Iteration 69, loss = 0.11649904\n",
      "Iteration 70, loss = 0.11596550\n",
      "Iteration 71, loss = 0.11583646\n",
      "Iteration 72, loss = 0.11563002\n",
      "Iteration 73, loss = 0.11549077\n",
      "Iteration 74, loss = 0.11525665\n",
      "Iteration 75, loss = 0.11499788\n",
      "Iteration 76, loss = 0.11513540\n",
      "Iteration 77, loss = 0.11477172\n",
      "Iteration 78, loss = 0.11456775\n",
      "Iteration 79, loss = 0.11467869\n",
      "Iteration 80, loss = 0.11450281\n",
      "Iteration 81, loss = 0.11428103\n",
      "Iteration 82, loss = 0.11399530\n",
      "Iteration 83, loss = 0.11398798\n",
      "Iteration 84, loss = 0.11348334\n",
      "Iteration 85, loss = 0.11384964\n",
      "Iteration 86, loss = 0.11335772\n",
      "Iteration 87, loss = 0.11351136\n",
      "Iteration 88, loss = 0.11342717\n",
      "Iteration 89, loss = 0.11308269\n",
      "Iteration 90, loss = 0.11271584\n",
      "Iteration 91, loss = 0.11405581\n",
      "Iteration 92, loss = 0.11281724\n",
      "Iteration 93, loss = 0.11272809\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44583786\n",
      "Iteration 2, loss = 0.30180687\n",
      "Iteration 3, loss = 0.27395918\n",
      "Iteration 4, loss = 0.25571461\n",
      "Iteration 5, loss = 0.24316690\n",
      "Iteration 6, loss = 0.23598847\n",
      "Iteration 7, loss = 0.23389972\n",
      "Iteration 8, loss = 0.22808155\n",
      "Iteration 9, loss = 0.22009682\n",
      "Iteration 10, loss = 0.21589283\n",
      "Iteration 11, loss = 0.21056341\n",
      "Iteration 12, loss = 0.20662943\n",
      "Iteration 13, loss = 0.20183075\n",
      "Iteration 14, loss = 0.19796775\n",
      "Iteration 15, loss = 0.19525907\n",
      "Iteration 16, loss = 0.19205367\n",
      "Iteration 17, loss = 0.19201538\n",
      "Iteration 18, loss = 0.23144887\n",
      "Iteration 19, loss = 0.20933579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33880236\n",
      "Iteration 2, loss = 0.24450130\n",
      "Iteration 3, loss = 0.22517905\n",
      "Iteration 4, loss = 0.21088918\n",
      "Iteration 5, loss = 0.20364217\n",
      "Iteration 6, loss = 0.19444202\n",
      "Iteration 7, loss = 0.19051274\n",
      "Iteration 8, loss = 0.19087231\n",
      "Iteration 9, loss = 0.18728284\n",
      "Iteration 10, loss = 0.18225650\n",
      "Iteration 11, loss = 0.17643411\n",
      "Iteration 12, loss = 0.17267322\n",
      "Iteration 13, loss = 0.17017041\n",
      "Iteration 14, loss = 0.16765794\n",
      "Iteration 15, loss = 0.17099849\n",
      "Iteration 16, loss = 0.16482168\n",
      "Iteration 17, loss = 0.16621401\n",
      "Iteration 18, loss = 0.16600832\n",
      "Iteration 19, loss = 0.16192104\n",
      "Iteration 20, loss = 0.16103901\n",
      "Iteration 21, loss = 0.16126009\n",
      "Iteration 22, loss = 0.15798177\n",
      "Iteration 23, loss = 0.15646621\n",
      "Iteration 24, loss = 0.15572854\n",
      "Iteration 25, loss = 0.15485870\n",
      "Iteration 26, loss = 0.15157084\n",
      "Iteration 27, loss = 0.15115226\n",
      "Iteration 28, loss = 0.15423361\n",
      "Iteration 29, loss = 0.16350685\n",
      "Iteration 30, loss = 0.15013039\n",
      "Iteration 31, loss = 0.14834099\n",
      "Iteration 32, loss = 0.14785622\n",
      "Iteration 33, loss = 0.14747528\n",
      "Iteration 34, loss = 0.14603639\n",
      "Iteration 35, loss = 0.14656965\n",
      "Iteration 36, loss = 0.14510272\n",
      "Iteration 37, loss = 0.14357042\n",
      "Iteration 38, loss = 0.14331410\n",
      "Iteration 39, loss = 0.14273752\n",
      "Iteration 40, loss = 0.14106632\n",
      "Iteration 41, loss = 0.14227161\n",
      "Iteration 42, loss = 0.14085699\n",
      "Iteration 43, loss = 0.14003942\n",
      "Iteration 44, loss = 0.13999521\n",
      "Iteration 45, loss = 0.13978857\n",
      "Iteration 46, loss = 0.13877949\n",
      "Iteration 47, loss = 0.13832843\n",
      "Iteration 48, loss = 0.13778623\n",
      "Iteration 49, loss = 0.13710522\n",
      "Iteration 50, loss = 0.13668341\n",
      "Iteration 51, loss = 0.13611766\n",
      "Iteration 52, loss = 0.13570319\n",
      "Iteration 53, loss = 0.13767552\n",
      "Iteration 54, loss = 0.13682969\n",
      "Iteration 55, loss = 0.13464624\n",
      "Iteration 56, loss = 0.13525932\n",
      "Iteration 57, loss = 0.13449371\n",
      "Iteration 58, loss = 0.13370438\n",
      "Iteration 59, loss = 0.13331145\n",
      "Iteration 60, loss = 0.13238853\n",
      "Iteration 61, loss = 0.13262592\n",
      "Iteration 62, loss = 0.13166768\n",
      "Iteration 63, loss = 0.13181377\n",
      "Iteration 64, loss = 0.13176613\n",
      "Iteration 65, loss = 0.13092049\n",
      "Iteration 66, loss = 0.13085175\n",
      "Iteration 67, loss = 0.13102859\n",
      "Iteration 68, loss = 0.13080193\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30494359\n",
      "Iteration 2, loss = 0.23313013\n",
      "Iteration 3, loss = 0.21091326\n",
      "Iteration 4, loss = 0.19463450\n",
      "Iteration 5, loss = 0.19057740\n",
      "Iteration 6, loss = 0.18183755\n",
      "Iteration 7, loss = 0.17628885\n",
      "Iteration 8, loss = 0.16934445\n",
      "Iteration 9, loss = 0.16435869\n",
      "Iteration 10, loss = 0.16753502\n",
      "Iteration 11, loss = 0.16067018\n",
      "Iteration 12, loss = 0.15728518\n",
      "Iteration 13, loss = 0.15468791\n",
      "Iteration 14, loss = 0.15329329\n",
      "Iteration 15, loss = 0.15104575\n",
      "Iteration 16, loss = 0.15051599\n",
      "Iteration 17, loss = 0.14849569\n",
      "Iteration 18, loss = 0.14717729\n",
      "Iteration 19, loss = 0.14684242\n",
      "Iteration 20, loss = 0.14517970\n",
      "Iteration 21, loss = 0.14478552\n",
      "Iteration 22, loss = 0.14673598\n",
      "Iteration 23, loss = 0.14341733\n",
      "Iteration 24, loss = 0.14140837\n",
      "Iteration 25, loss = 0.14203655\n",
      "Iteration 26, loss = 0.13967493\n",
      "Iteration 27, loss = 0.13964413\n",
      "Iteration 28, loss = 0.13930791\n",
      "Iteration 29, loss = 0.13909462\n",
      "Iteration 30, loss = 0.13755737\n",
      "Iteration 31, loss = 0.13614884\n",
      "Iteration 32, loss = 0.13617383\n",
      "Iteration 33, loss = 0.13564294\n",
      "Iteration 34, loss = 0.13568730\n",
      "Iteration 35, loss = 0.13424985\n",
      "Iteration 36, loss = 0.13396200\n",
      "Iteration 37, loss = 0.13498768\n",
      "Iteration 38, loss = 0.13247077\n",
      "Iteration 39, loss = 0.13317741\n",
      "Iteration 40, loss = 0.13265481\n",
      "Iteration 41, loss = 0.13196873\n",
      "Iteration 42, loss = 0.13142827\n",
      "Iteration 43, loss = 0.13132753\n",
      "Iteration 44, loss = 0.13170570\n",
      "Iteration 45, loss = 0.13023956\n",
      "Iteration 46, loss = 0.12982124\n",
      "Iteration 47, loss = 0.12982668\n",
      "Iteration 48, loss = 0.13324810\n",
      "Iteration 49, loss = 0.13020096\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27143851\n",
      "Iteration 2, loss = 0.20489435\n",
      "Iteration 3, loss = 0.18696488\n",
      "Iteration 4, loss = 0.17693297\n",
      "Iteration 5, loss = 0.16992341\n",
      "Iteration 6, loss = 0.16716039\n",
      "Iteration 7, loss = 0.16057034\n",
      "Iteration 8, loss = 0.15823985\n",
      "Iteration 9, loss = 0.15797946\n",
      "Iteration 10, loss = 0.15314181\n",
      "Iteration 11, loss = 0.15079249\n",
      "Iteration 12, loss = 0.14835658\n",
      "Iteration 13, loss = 0.14854320\n",
      "Iteration 14, loss = 0.14489209\n",
      "Iteration 15, loss = 0.14323317\n",
      "Iteration 16, loss = 0.14215013\n",
      "Iteration 17, loss = 0.14036859\n",
      "Iteration 18, loss = 0.14023861\n",
      "Iteration 19, loss = 0.13811980\n",
      "Iteration 20, loss = 0.13759656\n",
      "Iteration 21, loss = 0.13642017\n",
      "Iteration 22, loss = 0.13576087\n",
      "Iteration 23, loss = 0.13458990\n",
      "Iteration 24, loss = 0.13394212\n",
      "Iteration 25, loss = 0.13354373\n",
      "Iteration 26, loss = 0.13597794\n",
      "Iteration 27, loss = 0.13417171\n",
      "Iteration 28, loss = 0.13150193\n",
      "Iteration 29, loss = 0.13135614\n",
      "Iteration 30, loss = 0.13086570\n",
      "Iteration 31, loss = 0.13105779\n",
      "Iteration 32, loss = 0.12945156\n",
      "Iteration 33, loss = 0.13085653\n",
      "Iteration 34, loss = 0.13051824\n",
      "Iteration 35, loss = 0.12901858\n",
      "Iteration 36, loss = 0.12796802\n",
      "Iteration 37, loss = 0.12796317\n",
      "Iteration 38, loss = 0.12762482\n",
      "Iteration 39, loss = 0.12703155\n",
      "Iteration 40, loss = 0.12663099\n",
      "Iteration 41, loss = 0.13004486\n",
      "Iteration 42, loss = 0.12737262\n",
      "Iteration 43, loss = 0.12650148\n",
      "Iteration 44, loss = 0.12559929\n",
      "Iteration 45, loss = 0.12494159\n",
      "Iteration 46, loss = 0.12508658\n",
      "Iteration 47, loss = 0.12582493\n",
      "Iteration 48, loss = 0.12428745\n",
      "Iteration 49, loss = 0.12431564\n",
      "Iteration 50, loss = 0.12552143\n",
      "Iteration 51, loss = 0.12372345\n",
      "Iteration 52, loss = 0.12418235\n",
      "Iteration 53, loss = 0.12304823\n",
      "Iteration 54, loss = 0.12262551\n",
      "Iteration 55, loss = 0.12214283\n",
      "Iteration 56, loss = 0.12179742\n",
      "Iteration 57, loss = 0.12174292\n",
      "Iteration 58, loss = 0.12252413\n",
      "Iteration 59, loss = 0.12113735\n",
      "Iteration 60, loss = 0.12096689\n",
      "Iteration 61, loss = 0.12091359\n",
      "Iteration 62, loss = 0.12008431\n",
      "Iteration 63, loss = 0.12031111\n",
      "Iteration 64, loss = 0.12007339\n",
      "Iteration 65, loss = 0.12112177\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26252515\n",
      "Iteration 2, loss = 0.20041228\n",
      "Iteration 3, loss = 0.18366071\n",
      "Iteration 4, loss = 0.17306876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.16590501\n",
      "Iteration 6, loss = 0.16119033\n",
      "Iteration 7, loss = 0.15655927\n",
      "Iteration 8, loss = 0.15354465\n",
      "Iteration 9, loss = 0.15087749\n",
      "Iteration 10, loss = 0.14756350\n",
      "Iteration 11, loss = 0.15033819\n",
      "Iteration 12, loss = 0.14501617\n",
      "Iteration 13, loss = 0.14362292\n",
      "Iteration 14, loss = 0.14350246\n",
      "Iteration 15, loss = 0.14199519\n",
      "Iteration 16, loss = 0.14011112\n",
      "Iteration 17, loss = 0.13895009\n",
      "Iteration 18, loss = 0.13890754\n",
      "Iteration 19, loss = 0.13794670\n",
      "Iteration 20, loss = 0.13542919\n",
      "Iteration 21, loss = 0.13535843\n",
      "Iteration 22, loss = 0.13388416\n",
      "Iteration 23, loss = 0.13394790\n",
      "Iteration 24, loss = 0.13375114\n",
      "Iteration 25, loss = 0.13122808\n",
      "Iteration 26, loss = 0.13096659\n",
      "Iteration 27, loss = 0.12999781\n",
      "Iteration 28, loss = 0.13034634\n",
      "Iteration 29, loss = 0.13065507\n",
      "Iteration 30, loss = 0.12915469\n",
      "Iteration 31, loss = 0.12794497\n",
      "Iteration 32, loss = 0.12793618\n",
      "Iteration 33, loss = 0.12736701\n",
      "Iteration 34, loss = 0.12685846\n",
      "Iteration 35, loss = 0.12600054\n",
      "Iteration 36, loss = 0.12592395\n",
      "Iteration 37, loss = 0.12571676\n",
      "Iteration 38, loss = 0.12452896\n",
      "Iteration 39, loss = 0.12464079\n",
      "Iteration 40, loss = 0.12541839\n",
      "Iteration 41, loss = 0.12385584\n",
      "Iteration 42, loss = 0.12336696\n",
      "Iteration 43, loss = 0.12334823\n",
      "Iteration 44, loss = 0.12258025\n",
      "Iteration 45, loss = 0.12197681\n",
      "Iteration 46, loss = 0.12242997\n",
      "Iteration 47, loss = 0.12186269\n",
      "Iteration 48, loss = 0.12176706\n",
      "Iteration 49, loss = 0.12071919\n",
      "Iteration 50, loss = 0.12162894\n",
      "Iteration 51, loss = 0.12045972\n",
      "Iteration 52, loss = 0.12008171\n",
      "Iteration 53, loss = 0.11997598\n",
      "Iteration 54, loss = 0.12002248\n",
      "Iteration 55, loss = 0.12015246\n",
      "Iteration 56, loss = 0.12109828\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44583786\n",
      "Iteration 2, loss = 0.30180687\n",
      "Iteration 3, loss = 0.27395918\n",
      "Iteration 4, loss = 0.25571461\n",
      "Iteration 5, loss = 0.24316690\n",
      "Iteration 6, loss = 0.23598847\n",
      "Iteration 7, loss = 0.23389972\n",
      "Iteration 8, loss = 0.22808155\n",
      "Iteration 9, loss = 0.22009682\n",
      "Iteration 10, loss = 0.21589283\n",
      "Iteration 11, loss = 0.21056341\n",
      "Iteration 12, loss = 0.20662943\n",
      "Iteration 13, loss = 0.20183075\n",
      "Iteration 14, loss = 0.19796775\n",
      "Iteration 15, loss = 0.19525907\n",
      "Iteration 16, loss = 0.19205367\n",
      "Iteration 17, loss = 0.19201538\n",
      "Iteration 18, loss = 0.23144887\n",
      "Iteration 19, loss = 0.20933579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33880236\n",
      "Iteration 2, loss = 0.24450130\n",
      "Iteration 3, loss = 0.22517905\n",
      "Iteration 4, loss = 0.21088918\n",
      "Iteration 5, loss = 0.20364217\n",
      "Iteration 6, loss = 0.19444202\n",
      "Iteration 7, loss = 0.19051274\n",
      "Iteration 8, loss = 0.19087231\n",
      "Iteration 9, loss = 0.18728284\n",
      "Iteration 10, loss = 0.18225650\n",
      "Iteration 11, loss = 0.17643411\n",
      "Iteration 12, loss = 0.17267322\n",
      "Iteration 13, loss = 0.17017041\n",
      "Iteration 14, loss = 0.16765794\n",
      "Iteration 15, loss = 0.17099849\n",
      "Iteration 16, loss = 0.16482168\n",
      "Iteration 17, loss = 0.16621401\n",
      "Iteration 18, loss = 0.16600832\n",
      "Iteration 19, loss = 0.16192104\n",
      "Iteration 20, loss = 0.16103901\n",
      "Iteration 21, loss = 0.16126009\n",
      "Iteration 22, loss = 0.15798177\n",
      "Iteration 23, loss = 0.15646621\n",
      "Iteration 24, loss = 0.15572854\n",
      "Iteration 25, loss = 0.15485870\n",
      "Iteration 26, loss = 0.15157084\n",
      "Iteration 27, loss = 0.15115226\n",
      "Iteration 28, loss = 0.15423361\n",
      "Iteration 29, loss = 0.16350685\n",
      "Iteration 30, loss = 0.15013039\n",
      "Iteration 31, loss = 0.14834099\n",
      "Iteration 32, loss = 0.14785622\n",
      "Iteration 33, loss = 0.14747528\n",
      "Iteration 34, loss = 0.14603639\n",
      "Iteration 35, loss = 0.14656965\n",
      "Iteration 36, loss = 0.14510272\n",
      "Iteration 37, loss = 0.14357042\n",
      "Iteration 38, loss = 0.14331410\n",
      "Iteration 39, loss = 0.14273752\n",
      "Iteration 40, loss = 0.14106632\n",
      "Iteration 41, loss = 0.14227161\n",
      "Iteration 42, loss = 0.14085699\n",
      "Iteration 43, loss = 0.14003942\n",
      "Iteration 44, loss = 0.13999521\n",
      "Iteration 45, loss = 0.13978857\n",
      "Iteration 46, loss = 0.13877949\n",
      "Iteration 47, loss = 0.13832843\n",
      "Iteration 48, loss = 0.13778623\n",
      "Iteration 49, loss = 0.13710522\n",
      "Iteration 50, loss = 0.13668341\n",
      "Iteration 51, loss = 0.13611766\n",
      "Iteration 52, loss = 0.13570319\n",
      "Iteration 53, loss = 0.13767552\n",
      "Iteration 54, loss = 0.13682969\n",
      "Iteration 55, loss = 0.13464624\n",
      "Iteration 56, loss = 0.13525932\n",
      "Iteration 57, loss = 0.13449371\n",
      "Iteration 58, loss = 0.13370438\n",
      "Iteration 59, loss = 0.13331145\n",
      "Iteration 60, loss = 0.13238853\n",
      "Iteration 61, loss = 0.13262592\n",
      "Iteration 62, loss = 0.13166768\n",
      "Iteration 63, loss = 0.13181377\n",
      "Iteration 64, loss = 0.13176613\n",
      "Iteration 65, loss = 0.13092049\n",
      "Iteration 66, loss = 0.13085175\n",
      "Iteration 67, loss = 0.13102859\n",
      "Iteration 68, loss = 0.13080193\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30287131\n",
      "Iteration 2, loss = 0.23003254\n",
      "Iteration 3, loss = 0.20461108\n",
      "Iteration 4, loss = 0.19759120\n",
      "Iteration 5, loss = 0.19451609\n",
      "Iteration 6, loss = 0.18228621\n",
      "Iteration 7, loss = 0.17929293\n",
      "Iteration 8, loss = 0.17494226\n",
      "Iteration 9, loss = 0.17219811\n",
      "Iteration 10, loss = 0.16731371\n",
      "Iteration 11, loss = 0.16528271\n",
      "Iteration 12, loss = 0.16690453\n",
      "Iteration 13, loss = 0.17051389\n",
      "Iteration 14, loss = 0.15959030\n",
      "Iteration 15, loss = 0.15726340\n",
      "Iteration 16, loss = 0.15643481\n",
      "Iteration 17, loss = 0.15451139\n",
      "Iteration 18, loss = 0.15181017\n",
      "Iteration 19, loss = 0.14997285\n",
      "Iteration 20, loss = 0.14882513\n",
      "Iteration 21, loss = 0.14837020\n",
      "Iteration 22, loss = 0.14613924\n",
      "Iteration 23, loss = 0.14521018\n",
      "Iteration 24, loss = 0.14428171\n",
      "Iteration 25, loss = 0.14420259\n",
      "Iteration 26, loss = 0.14286656\n",
      "Iteration 27, loss = 0.14316161\n",
      "Iteration 28, loss = 0.14155957\n",
      "Iteration 29, loss = 0.14007093\n",
      "Iteration 30, loss = 0.13922506\n",
      "Iteration 31, loss = 0.13808973\n",
      "Iteration 32, loss = 0.13753874\n",
      "Iteration 33, loss = 0.13732498\n",
      "Iteration 34, loss = 0.13663939\n",
      "Iteration 35, loss = 0.13707255\n",
      "Iteration 36, loss = 0.13581660\n",
      "Iteration 37, loss = 0.13493144\n",
      "Iteration 38, loss = 0.13436637\n",
      "Iteration 39, loss = 0.13598495\n",
      "Iteration 40, loss = 0.13387803\n",
      "Iteration 41, loss = 0.13312800\n",
      "Iteration 42, loss = 0.13267035\n",
      "Iteration 43, loss = 0.13210650\n",
      "Iteration 44, loss = 0.13214260\n",
      "Iteration 45, loss = 0.13157149\n",
      "Iteration 46, loss = 0.13110751\n",
      "Iteration 47, loss = 0.13117639\n",
      "Iteration 48, loss = 0.12997241\n",
      "Iteration 49, loss = 0.13007915\n",
      "Iteration 50, loss = 0.13079150\n",
      "Iteration 51, loss = 0.12976521\n",
      "Iteration 52, loss = 0.12889460\n",
      "Iteration 53, loss = 0.12906693\n",
      "Iteration 54, loss = 0.12909579\n",
      "Iteration 55, loss = 0.12837573\n",
      "Iteration 56, loss = 0.12831444\n",
      "Iteration 57, loss = 0.12748810\n",
      "Iteration 58, loss = 0.12809839\n",
      "Iteration 59, loss = 0.12792905\n",
      "Iteration 60, loss = 0.12716896\n",
      "Iteration 61, loss = 0.12694380\n",
      "Iteration 62, loss = 0.12632547\n",
      "Iteration 63, loss = 0.12710745\n",
      "Iteration 64, loss = 0.12576923\n",
      "Iteration 65, loss = 0.12563390\n",
      "Iteration 66, loss = 0.12525443\n",
      "Iteration 67, loss = 0.12985547\n",
      "Iteration 68, loss = 0.12621219\n",
      "Iteration 69, loss = 0.12523149\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27165729\n",
      "Iteration 2, loss = 0.20708439\n",
      "Iteration 3, loss = 0.19225377\n",
      "Iteration 4, loss = 0.18055560\n",
      "Iteration 5, loss = 0.17542460\n",
      "Iteration 6, loss = 0.16996588\n",
      "Iteration 7, loss = 0.16464174\n",
      "Iteration 8, loss = 0.16030541\n",
      "Iteration 9, loss = 0.15709585\n",
      "Iteration 10, loss = 0.15649603\n",
      "Iteration 11, loss = 0.15272593\n",
      "Iteration 12, loss = 0.15499878\n",
      "Iteration 13, loss = 0.14773988\n",
      "Iteration 14, loss = 0.14694829\n",
      "Iteration 15, loss = 0.14514946\n",
      "Iteration 16, loss = 0.14312145\n",
      "Iteration 17, loss = 0.14215864\n",
      "Iteration 18, loss = 0.14072800\n",
      "Iteration 19, loss = 0.14713733\n",
      "Iteration 20, loss = 0.13961217\n",
      "Iteration 21, loss = 0.13829624\n",
      "Iteration 22, loss = 0.13805210\n",
      "Iteration 23, loss = 0.13623173\n",
      "Iteration 24, loss = 0.13748830\n",
      "Iteration 25, loss = 0.13437495\n",
      "Iteration 26, loss = 0.13701397\n",
      "Iteration 27, loss = 0.13423770\n",
      "Iteration 28, loss = 0.13324769\n",
      "Iteration 29, loss = 0.13269446\n",
      "Iteration 30, loss = 0.13201952\n",
      "Iteration 31, loss = 0.13173446\n",
      "Iteration 32, loss = 0.13271355\n",
      "Iteration 33, loss = 0.12945165\n",
      "Iteration 34, loss = 0.13026082\n",
      "Iteration 35, loss = 0.12882102\n",
      "Iteration 36, loss = 0.12921268\n",
      "Iteration 37, loss = 0.12851314\n",
      "Iteration 38, loss = 0.12815051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.12775018\n",
      "Iteration 40, loss = 0.12744253\n",
      "Iteration 41, loss = 0.12663216\n",
      "Iteration 42, loss = 0.12717408\n",
      "Iteration 43, loss = 0.12621238\n",
      "Iteration 44, loss = 0.12548526\n",
      "Iteration 45, loss = 0.12561272\n",
      "Iteration 46, loss = 0.12525440\n",
      "Iteration 47, loss = 0.12634773\n",
      "Iteration 48, loss = 0.12616526\n",
      "Iteration 49, loss = 0.12415369\n",
      "Iteration 50, loss = 0.12359744\n",
      "Iteration 51, loss = 0.12487056\n",
      "Iteration 52, loss = 0.12291413\n",
      "Iteration 53, loss = 0.12265515\n",
      "Iteration 54, loss = 0.12256070\n",
      "Iteration 55, loss = 0.12252518\n",
      "Iteration 56, loss = 0.12204123\n",
      "Iteration 57, loss = 0.12201260\n",
      "Iteration 58, loss = 0.12146790\n",
      "Iteration 59, loss = 0.12268904\n",
      "Iteration 60, loss = 0.12099139\n",
      "Iteration 61, loss = 0.12237149\n",
      "Iteration 62, loss = 0.12025076\n",
      "Iteration 63, loss = 0.12102708\n",
      "Iteration 64, loss = 0.12097402\n",
      "Iteration 65, loss = 0.12024647\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25616499\n",
      "Iteration 2, loss = 0.19746843\n",
      "Iteration 3, loss = 0.18249669\n",
      "Iteration 4, loss = 0.17029016\n",
      "Iteration 5, loss = 0.17024053\n",
      "Iteration 6, loss = 0.16253750\n",
      "Iteration 7, loss = 0.15573518\n",
      "Iteration 8, loss = 0.15204266\n",
      "Iteration 9, loss = 0.14884397\n",
      "Iteration 10, loss = 0.14622234\n",
      "Iteration 11, loss = 0.14551962\n",
      "Iteration 12, loss = 0.14235886\n",
      "Iteration 13, loss = 0.14076196\n",
      "Iteration 14, loss = 0.14074890\n",
      "Iteration 15, loss = 0.13801935\n",
      "Iteration 16, loss = 0.13715772\n",
      "Iteration 17, loss = 0.13752537\n",
      "Iteration 18, loss = 0.13493458\n",
      "Iteration 19, loss = 0.13420097\n",
      "Iteration 20, loss = 0.13258209\n",
      "Iteration 21, loss = 0.13146524\n",
      "Iteration 22, loss = 0.13054934\n",
      "Iteration 23, loss = 0.13267655\n",
      "Iteration 24, loss = 0.12921725\n",
      "Iteration 25, loss = 0.12854450\n",
      "Iteration 26, loss = 0.12806397\n",
      "Iteration 27, loss = 0.12770311\n",
      "Iteration 28, loss = 0.12647399\n",
      "Iteration 29, loss = 0.12579971\n",
      "Iteration 30, loss = 0.12592823\n",
      "Iteration 31, loss = 0.12470913\n",
      "Iteration 32, loss = 0.12587785\n",
      "Iteration 33, loss = 0.12440240\n",
      "Iteration 34, loss = 0.12380843\n",
      "Iteration 35, loss = 0.12361587\n",
      "Iteration 36, loss = 0.12247815\n",
      "Iteration 37, loss = 0.12246863\n",
      "Iteration 38, loss = 0.12342630\n",
      "Iteration 39, loss = 0.12179425\n",
      "Iteration 40, loss = 0.12130193\n",
      "Iteration 41, loss = 0.12054491\n",
      "Iteration 42, loss = 0.12108479\n",
      "Iteration 43, loss = 0.12030924\n",
      "Iteration 44, loss = 0.12033564\n",
      "Iteration 45, loss = 0.11978836\n",
      "Iteration 46, loss = 0.11924432\n",
      "Iteration 47, loss = 0.11926137\n",
      "Iteration 48, loss = 0.11874102\n",
      "Iteration 49, loss = 0.11845832\n",
      "Iteration 50, loss = 0.11841497\n",
      "Iteration 51, loss = 0.11805349\n",
      "Iteration 52, loss = 0.11742624\n",
      "Iteration 53, loss = 0.11773943\n",
      "Iteration 54, loss = 0.11738463\n",
      "Iteration 55, loss = 0.11698122\n",
      "Iteration 56, loss = 0.11716192\n",
      "Iteration 57, loss = 0.11630191\n",
      "Iteration 58, loss = 0.11627141\n",
      "Iteration 59, loss = 0.11619452\n",
      "Iteration 60, loss = 0.11600862\n",
      "Iteration 61, loss = 0.11626555\n",
      "Iteration 62, loss = 0.11521406\n",
      "Iteration 63, loss = 0.11545292\n",
      "Iteration 64, loss = 0.11478756\n",
      "Iteration 65, loss = 0.11817357\n",
      "Iteration 66, loss = 0.11655294\n",
      "Iteration 67, loss = 0.11477625\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2238002fda0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nam7QCw9pZREAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22380c887b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot Learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import learning_curve\n",
    "plt.figure()\n",
    "plt.title('Learning')\n",
    "ylim = None\n",
    "if ylim is not None:\n",
    "    plt.ylim(*ylim)\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    clf, train, train_labels, cv=None, n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}